{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fdf509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e17ae1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"../prepared/instruction_dataset.jsonl\"\n",
    "TOKENIZER_FILE = \"../tokens/tokenizer.json\"\n",
    "CHECKPOINT_DIR = \"checkpoints_inst\"  \n",
    "PRETRAINED_CKPT = \"checkpoints/ckpt_step_8070.pt\" \n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e66059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 16000\n",
    "DIM = 256\n",
    "NUM_LAYERS = 6\n",
    "NUM_HEADS = 8\n",
    "FFN_DIM = 1024\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "# TUNING HYPERPARAMETERS (Lower LR, fewer epochs)\n",
    "PHYSICAL_BATCH = 1\n",
    "GRAD_ACCUM = 32\n",
    "EPOCHS = 5      # 3-5 is usually enough for tuning\n",
    "LR = 2e-5       # 10x smaller than pre-training\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOG_EVERY = 10\n",
    "SAVE_EVERY = 10000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87d6784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):  \n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Force float32 for norm calculation to avoid overflow\n",
    "        x_f32 = x.float()\n",
    "        norm = x_f32.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4c91c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    x1 = x[..., :x.shape[-1]//2]\n",
    "    x2 = x[..., x.shape[-1]//2:]\n",
    "    return torch.cat([-x2, x1], dim=-1)\n",
    "\n",
    "def apply_rope(x, rope_sin, rope_cos):\n",
    "    return (x * rope_cos) + (rotate_half(x) * rope_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646187b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, mask, rope_sin, rope_cos):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)  # (B, T, 3*C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # reshape -> (B, heads, T, head_dim)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # apply RoPE to q,k\n",
    "        q = apply_rope(q, rope_sin, rope_cos)\n",
    "        k = apply_rope(k, rope_sin, rope_cos)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "\n",
    "        y = att @ v  # (B, heads, T, head_dim)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        return self.proj(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1880d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(dim, hidden_dim)\n",
    "        self.w3 = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w3(torch.nn.functional.silu(self.w1(x)) * self.w2(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e44803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(dim)\n",
    "        self.attn = MultiHeadSelfAttention(dim, num_heads)\n",
    "        self.norm2 = RMSNorm(dim)\n",
    "        self.ffn = SwiGLU(dim, ffn_dim)\n",
    "\n",
    "    def forward(self, x, mask, rope_sin, rope_cos):\n",
    "        x = x + self.attn(self.norm1(x), mask, rope_sin, rope_cos)\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e677c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, num_layers, num_heads, ffn_dim, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = RMSNorm(dim)\n",
    "        self.lm_head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "        # weight tying\n",
    "        self.lm_head.weight = self.token_emb.weight\n",
    "\n",
    "        pos = torch.arange(max_seq_len)  # [T]\n",
    "        freqs = 1.0 / (10000 ** (torch.arange(0, self.head_dim, 2) / self.head_dim))\n",
    "        # freqs len = head_dim/2\n",
    "\n",
    "        sinusoid = torch.einsum(\"i,j->ij\", pos, freqs)  # [T, head_dim/2]\n",
    "\n",
    "        rope_sin = sinusoid.sin()   # [T, head_dim/2]\n",
    "        rope_cos = sinusoid.cos()   # [T, head_dim/2]\n",
    "\n",
    "        # Expand to: [1, 1, T, head_dim]\n",
    "        rope_sin = torch.cat([rope_sin, rope_sin], dim=-1)\n",
    "        rope_cos = torch.cat([rope_cos, rope_cos], dim=-1)\n",
    "\n",
    "        self.register_buffer(\"rope_sin\", rope_sin.unsqueeze(0).unsqueeze(0))\n",
    "        self.register_buffer(\"rope_cos\", rope_cos.unsqueeze(0).unsqueeze(0))\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, mask):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        x = self.token_emb(idx)\n",
    "\n",
    "        # Correct slice: [1,1,T,head_dim]\n",
    "        rope_sin = self.rope_sin[:, :, :T, :]\n",
    "        rope_cos = self.rope_cos[:, :, :T, :]\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mask, rope_sin, rope_cos)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da47b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(T, device):\n",
    "    m = torch.tril(torch.ones((T, T), dtype=torch.bool, device=device))\n",
    "    return m.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15b41ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstrDataset(Dataset):\n",
    "    def __init__(self, fname, tok, seq=MAX_SEQ_LEN):\n",
    "        self.rows = []\n",
    "        self.eos_id = tok.token_to_id(\"</s>\")\n",
    "        if self.eos_id is None: self.eos_id = tok.token_to_id(\"<|endoftext|>\")\n",
    "        if self.eos_id is None: self.eos_id = 0 # Fallback\n",
    "\n",
    "        print(f\"Loading {fname}...\")\n",
    "        with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "            for ln in f:\n",
    "                if not ln.strip(): continue\n",
    "                o = json.loads(ln)\n",
    "                \n",
    "               \n",
    "                instr = o.get(\"instruction\",\"\").strip()\n",
    "                inp = o.get(\"input\",\"\").strip()\n",
    "                out = o.get(\"output\",\"\").strip()\n",
    "\n",
    "                prompt_text = f\"{instr}\\n\\n{inp}\\n\\n### Response:\\n\"\n",
    "                \n",
    "                full_text = prompt_text + out\n",
    "\n",
    "                prompt_ids = tok.encode(prompt_text).ids\n",
    "                full_ids = tok.encode(full_text).ids\n",
    "\n",
    "                if not full_ids or full_ids[-1] != self.eos_id:\n",
    "                    full_ids.append(self.eos_id)\n",
    "\n",
    "                if len(full_ids) > seq: continue \n",
    "                \n",
    "                self.rows.append((prompt_ids, full_ids))\n",
    "        print(f\"Loaded {len(self.rows)} instruction samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        prompt_ids, full_ids = self.rows[i]\n",
    "        \n",
    "        # Prepare tensors\n",
    "        x = torch.tensor(full_ids, dtype=torch.long)\n",
    "        \n",
    "        # Labels: Same as X, but we mask the prompt\n",
    "        # -100 is PyTorch's \"Ignore Index\" for CrossEntropy\n",
    "        y = x.clone()\n",
    "        prompt_len = len(prompt_ids)\n",
    "        \n",
    "        # CRITICAL: Mask the prompt so we don't calculate loss on it\n",
    "        # We only want to learn to generate the OUTPUT\n",
    "        if prompt_len < len(y):\n",
    "            y[:prompt_len] = -100\n",
    "        else:\n",
    "            # Should not happen given logic, but safety\n",
    "            y[:] = -100\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad batch to longest sequence\n",
    "    xs, ys = zip(*batch)\n",
    "    max_len = max(len(x) for x in xs)\n",
    "    \n",
    "    # Pad inputs with 0, targets with -100\n",
    "    X_pad = torch.full((len(xs), max_len), 0, dtype=torch.long)\n",
    "    Y_pad = torch.full((len(xs), max_len), -100, dtype=torch.long)\n",
    "    \n",
    "    for i, (x, y) in enumerate(zip(xs, ys)):\n",
    "        X_pad[i, :len(x)] = x\n",
    "        Y_pad[i, :len(y)] = y\n",
    "        \n",
    "    return X_pad, Y_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5078e7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../prepared/instruction_dataset.jsonl...\n",
      "Loaded 246525 instruction samples.\n",
      "Loading Base Model: checkpoints/ckpt_step_8070.pt\n",
      "Pre-trained weights loaded successfully.\n",
      "Starting Instruction Tuning on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 246525/246525 [2:56:43<00:00, 23.25it/s, inst_loss=0.3547]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished | Avg Loss: 0.4649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 246525/246525 [2:34:01<00:00, 26.68it/s, inst_loss=0.0031]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished | Avg Loss: 0.2791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 246525/246525 [2:33:53<00:00, 26.70it/s, inst_loss=0.6016]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished | Avg Loss: 0.1851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 246525/246525 [2:33:59<00:00, 26.68it/s, inst_loss=0.0000]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished | Avg Loss: 0.1574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 246525/246525 [2:34:03<00:00, 26.67it/s, inst_loss=0.0003]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished | Avg Loss: 0.1421\n",
      "Instruction Fine-Tuning Completed.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(TOKENIZER_FILE)\n",
    "ds = InstrDataset(DATA_FILE, tokenizer)\n",
    "loader = DataLoader(ds, batch_size=PHYSICAL_BATCH, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Loading Base Model: {PRETRAINED_CKPT}\")\n",
    "model = TransformerLM(VOCAB_SIZE, DIM, NUM_LAYERS, NUM_HEADS, FFN_DIM, MAX_SEQ_LEN).to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load(PRETRAINED_CKPT, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "print(\"Pre-trained weights loaded successfully.\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.amp.GradScaler(enabled=(DEVICE.type==\"cuda\"))\n",
    "\n",
    "print(f\"Starting Instruction Tuning on {DEVICE}...\")\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    epoch_loss_sum = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        mask = causal_mask(x.size(1), DEVICE)\n",
    "        \n",
    "        with torch.amp.autocast(device_type=\"cuda\" if DEVICE.type==\"cuda\" else \"cpu\"):\n",
    "            logits = model(x, mask)\n",
    "            loss = F.cross_entropy(logits[:, :-1, :].reshape(-1, VOCAB_SIZE), \n",
    "                                   y[:, 1:].reshape(-1), \n",
    "                                   ignore_index=-100)\n",
    "            loss = loss / GRAD_ACCUM\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        current_loss = loss.item() * GRAD_ACCUM\n",
    "        epoch_loss_sum += current_loss\n",
    "        num_batches += 1\n",
    "        \n",
    "        if (global_step + 1) % GRAD_ACCUM == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        global_step += 1\n",
    "\n",
    "        if global_step % LOG_EVERY == 0:\n",
    "            pbar.set_postfix({\"inst_loss\": f\"{current_loss:.4f}\"})\n",
    "        \n",
    "        if global_step % SAVE_EVERY == 0:\n",
    "            fname = os.path.join(CHECKPOINT_DIR, f\"inst_tune_step_{global_step}.pt\")\n",
    "            torch.save({\"model_state_dict\": model.state_dict()}, fname)\n",
    "\n",
    "    avg_loss = epoch_loss_sum / max(1, num_batches)\n",
    "    print(f\"Epoch {epoch+1} finished | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save({\"model_state_dict\": model.state_dict()}, os.path.join(CHECKPOINT_DIR, \"inst_tune_final.pt\"))\n",
    "print(\"Instruction Fine-Tuning Completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
